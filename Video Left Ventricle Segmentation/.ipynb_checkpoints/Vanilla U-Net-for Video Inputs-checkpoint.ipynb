{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\", class=\"alert alert-block alert-success\">\n",
    "    <h1>Deep Learning Project</h1>\n",
    "    <h2>Vanilla U-Net</h2>\n",
    "    <h3>Performing Left ventricle Segmentation on Videos</h3>\n",
    "    <h4><i>By Ariba Khan (17270) and Dr. Sawera Hanif (29413)</i></h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, we will import the trained **vanilla U-Net model** from the previous notebook which had learned on an initial dataset of **1,014** images and their corresponding masks.\n",
    "\n",
    "The objective is to evaluate the capability of the vanilla U-Net to accurately segment the left ventricle in echocardiogram videos, despite being trained on a constrained dataset with limited publicly available annotated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing All Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-08T15:56:43.840768Z",
     "iopub.status.busy": "2025-01-08T15:56:43.840446Z",
     "iopub.status.idle": "2025-01-08T15:56:50.320117Z",
     "shell.execute_reply": "2025-01-08T15:56:50.319212Z",
     "shell.execute_reply.started": "2025-01-08T15:56:43.840738Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=e57a5b782820d9eb44812e704e8fb655feaeaeb1b3119df5a4c95fa5c3bd8651\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil\n",
      "Successfully installed GPUtil-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-08T15:57:01.165192Z",
     "iopub.status.busy": "2025-01-08T15:57:01.164838Z",
     "iopub.status.idle": "2025-01-08T15:57:01.961171Z",
     "shell.execute_reply": "2025-01-08T15:57:01.960511Z",
     "shell.execute_reply.started": "2025-01-08T15:57:01.165159Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "from threading import Thread\n",
    "from sklearn.metrics import accuracy_score\n",
    "import zipfile\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Vanilla U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:59:22.566136Z",
     "iopub.status.busy": "2025-01-08T15:59:22.565809Z",
     "iopub.status.idle": "2025-01-08T15:59:22.574211Z",
     "shell.execute_reply": "2025-01-08T15:59:22.573383Z",
     "shell.execute_reply.started": "2025-01-08T15:59:22.566110Z"
    }
   },
   "outputs": [],
   "source": [
    "class VanillaUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(VanillaUNet, self).__init__()\n",
    "\n",
    "        # encoder blocks\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "\n",
    "        # pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # decoder blocks\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = self.conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = self.conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = self.conv_block(128, 64)\n",
    "\n",
    "        # final output layer\n",
    "        self.out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(self.pool(x1))\n",
    "        x3 = self.enc3(self.pool(x2))\n",
    "        x4 = self.enc4(self.pool(x3))\n",
    "\n",
    "        # decoder\n",
    "        x = self.upconv3(x4)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.dec3(x)\n",
    "\n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.dec2(x)\n",
    "\n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.dec1(x)\n",
    "\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring Resources During Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:57:56.393640Z",
     "iopub.status.busy": "2025-01-08T15:57:56.393350Z",
     "iopub.status.idle": "2025-01-08T15:57:56.398784Z",
     "shell.execute_reply": "2025-01-08T15:57:56.397876Z",
     "shell.execute_reply.started": "2025-01-08T15:57:56.393617Z"
    }
   },
   "outputs": [],
   "source": [
    "def monitor_resources_continuously(log):\n",
    "    process = psutil.Process()\n",
    "    while log[\"running\"]:\n",
    "        try:\n",
    "            # get memory usage\n",
    "            memory_info = process.memory_info()\n",
    "            memory_mb = memory_info.rss / (1024 * 1024) # convert to MB\n",
    "            \n",
    "            # get CPU usage\n",
    "            cpu_percent = psutil.cpu_percent(interval=0.1) # short interval for real-time updates\n",
    "            \n",
    "            # get GPU usage\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            gpu_percent = gpus[0].memoryUtil * 100 if gpus else 0 # GPU memory usage\n",
    "            \n",
    "            # logging the usage\n",
    "            log[\"memory\"].append(memory_mb)\n",
    "            log[\"cpu\"].append(cpu_percent)\n",
    "            log[\"gpu\"].append(gpu_percent)\n",
    "        except Exception as e:\n",
    "            print(f\"Resource monitoring error: {e}\")\n",
    "        time.sleep(0.5)  # log every 0.5 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Segmentation Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:17:21.234957Z",
     "iopub.status.busy": "2025-01-08T16:17:21.234581Z",
     "iopub.status.idle": "2025-01-08T16:17:21.243449Z",
     "shell.execute_reply": "2025-01-08T16:17:21.242518Z",
     "shell.execute_reply.started": "2025-01-08T16:17:21.234930Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions for Video Segmentation\n",
    "\n",
    "# 1. Function to apply mask to frame\n",
    "def apply_mask_to_frame(frame, mask):\n",
    "    \"\"\"Apply the mask to the frame.\"\"\"\n",
    "    # ensuring the frame and mask are of the same shape\n",
    "    mask_resized = cv2.resize(mask, (frame.shape[1], frame.shape[0]))\n",
    "    \n",
    "    # converting the mask to a 3-channel image (for visualization)\n",
    "    mask_colored = np.stack([mask_resized] * 3, axis=-1)\n",
    "    \n",
    "    # zeroing out the areas outside the left ventricle\n",
    "    masked_frame = frame * mask_colored\n",
    "    \n",
    "    # converting to uint8 format\n",
    "    masked_frame = np.clip(masked_frame, 0, 255).astype(np.uint8)\n",
    "    return masked_frame\n",
    "\n",
    "# 2. Function to segment left ventricle in each frame\n",
    "def segment_left_ventricle_in_video(input_video_path, output_dir, model, device, \n",
    "                                    original_video_name=\"Original.avi\", segmented_video_name=\"VanillaUNet.avi\"):\n",
    "    # defining output paths\n",
    "    original_output_path = os.path.join(output_dir, original_video_name)\n",
    "    segmented_output_path = os.path.join(output_dir, segmented_video_name)\n",
    "    \n",
    "    # opening the input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # getting video information\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # defining the codec and creating VideoWriter objects\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID') # for .avi files\n",
    "    out_original = cv2.VideoWriter(original_output_path, fourcc, fps, (frame_width, frame_height))\n",
    "    out_segmented = cv2.VideoWriter(segmented_output_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    # preparing the transformation for the model input\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # moving the model to the GPU before inference\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # writing the original frame to the \"Original\" video\n",
    "            out_original.write(frame)\n",
    "            \n",
    "            # preprocessing the frame\n",
    "            input_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            input_frame = transform(input_frame).unsqueeze(0).to(device)  # Add batch dimension\n",
    "            \n",
    "            # running the model on the frame\n",
    "            output_mask = model(input_frame)\n",
    "            output_mask = torch.sigmoid(output_mask).cpu().numpy().squeeze()  # Convert to numpy\n",
    "            \n",
    "            # applying the mask to the frame\n",
    "            segmented_frame = apply_mask_to_frame(frame, output_mask)\n",
    "            \n",
    "            # writing the processed frame to the \"VanillaUNet\" video\n",
    "            out_segmented.write(segmented_frame)\n",
    "        \n",
    "        cap.release()\n",
    "        out_original.release()\n",
    "        out_segmented.release()\n",
    "        print(f\"Original video saved to {original_output_path}\")\n",
    "        print(f\"Segmented video saved to {segmented_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Pre-Trained Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:59:42.947428Z",
     "iopub.status.busy": "2025-01-08T15:59:42.947142Z",
     "iopub.status.idle": "2025-01-08T15:59:44.074829Z",
     "shell.execute_reply": "2025-01-08T15:59:44.073883Z",
     "shell.execute_reply.started": "2025-01-08T15:59:42.947407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VanillaUNet(in_channels=3, out_channels=1)\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/vanilla-u-net-trained/pytorch/default/1/vanilla_unet.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\", class=\"alert alert-block alert-success\">\n",
    "    <h4>Inference Phase</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:25:30.835366Z",
     "iopub.status.busy": "2025-01-08T16:25:30.834924Z",
     "iopub.status.idle": "2025-01-08T16:25:30.839869Z",
     "shell.execute_reply": "2025-01-08T16:25:30.838819Z",
     "shell.execute_reply.started": "2025-01-08T16:25:30.835338Z"
    }
   },
   "outputs": [],
   "source": [
    "input_video_path = \"/kaggle/input/echonet-videos/EchoNet-Dynamic/Videos/0X1002E8FBACD08477.avi\"\n",
    "output_dir = \"/kaggle/working\"\n",
    "original_video_name = \"Video1_Original.avi\"  # custom name for the original video\n",
    "segmented_video_name = \"Video1_VanillaUNet.avi\"  # custom name for the segmented video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:25:34.242183Z",
     "iopub.status.busy": "2025-01-08T16:25:34.241852Z",
     "iopub.status.idle": "2025-01-08T16:25:35.523446Z",
     "shell.execute_reply": "2025-01-08T16:25:35.522541Z",
     "shell.execute_reply.started": "2025-01-08T16:25:34.242158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original video saved to /kaggle/working/Video1_Original.avi\n",
      "Segmented video saved to /kaggle/working/Video1_VanillaUNet.avi\n"
     ]
    }
   ],
   "source": [
    "# logging setup for resource monitoring\n",
    "log = {\"memory\": [], \"cpu\": [], \"gpu\": [], \"running\": True}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# starting resource monitoring in a separate thread\n",
    "monitor_thread = Thread(target=monitor_resources_continuously, args=(log,))\n",
    "monitor_thread.start()\n",
    "\n",
    "segment_left_ventricle_in_video(input_video_path, output_dir, model, device, \n",
    "                                original_video_name=original_video_name, \n",
    "                                segmented_video_name=segmented_video_name)\n",
    "\n",
    "# stopping resource monitoring after training is complete\n",
    "log[\"running\"] = False\n",
    "monitor_thread.join()\n",
    "\n",
    "# calculating training time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:25:40.442849Z",
     "iopub.status.busy": "2025-01-08T16:25:40.442534Z",
     "iopub.status.idle": "2025-01-08T16:25:40.450473Z",
     "shell.execute_reply": "2025-01-08T16:25:40.449721Z",
     "shell.execute_reply.started": "2025-01-08T16:25:40.442822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 1.28 seconds\n",
      "Average Memory Usage: 897.28 MB\n",
      "Peak Memory Usage: 897.28 MB\n",
      "Average CPU Usage: 58.25%\n",
      "Peak CPU Usage: 75.00%\n",
      "Average GPU Usage: 1.37%\n",
      "Peak GPU Usage: 1.37%\n"
     ]
    }
   ],
   "source": [
    "# recording resources used during inference\n",
    "\n",
    "if len(log[\"memory\"]) > 0:\n",
    "    avg_memory = sum(log[\"memory\"]) / len(log[\"memory\"])\n",
    "    peak_memory = max(log[\"memory\"])\n",
    "else:\n",
    "    avg_memory = peak_memory = 0\n",
    "\n",
    "if len(log[\"cpu\"]) > 0:\n",
    "    avg_cpu = sum(log[\"cpu\"]) / len(log[\"cpu\"])\n",
    "    peak_cpu = max(log[\"cpu\"])\n",
    "else:\n",
    "    avg_cpu = peak_cpu = 0\n",
    "\n",
    "if len(log[\"gpu\"]) > 0:\n",
    "    avg_gpu = sum(log[\"gpu\"]) / len(log[\"gpu\"])\n",
    "    peak_gpu = max(log[\"gpu\"])\n",
    "else:\n",
    "    avg_gpu = peak_gpu = 0\n",
    "\n",
    "print(f\"Inference Time: {inference_time:.2f} seconds\")\n",
    "print(f\"Average Memory Usage: {avg_memory:.2f} MB\")\n",
    "print(f\"Peak Memory Usage: {peak_memory:.2f} MB\")\n",
    "print(f\"Average CPU Usage: {avg_cpu:.2f}%\")\n",
    "print(f\"Peak CPU Usage: {peak_cpu:.2f}%\")\n",
    "print(f\"Average GPU Usage: {avg_gpu:.2f}%\")\n",
    "print(f\"Peak GPU Usage: {peak_gpu:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:26:12.018301Z",
     "iopub.status.busy": "2025-01-08T16:26:12.017958Z",
     "iopub.status.idle": "2025-01-08T16:26:12.021910Z",
     "shell.execute_reply": "2025-01-08T16:26:12.021234Z",
     "shell.execute_reply.started": "2025-01-08T16:26:12.018277Z"
    }
   },
   "outputs": [],
   "source": [
    "input_video_path = \"/kaggle/input/echonet-videos/EchoNet-Dynamic/Videos/0X1005D03EED19C65B.avi\"\n",
    "output_dir = \"/kaggle/working\"\n",
    "original_video_name = \"Video2_Original.avi\"  # custom name for the original video\n",
    "segmented_video_name = \"Video2_VanillaUNet.avi\"  # custom name for the segmented video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:26:16.947128Z",
     "iopub.status.busy": "2025-01-08T16:26:16.946790Z",
     "iopub.status.idle": "2025-01-08T16:26:17.587959Z",
     "shell.execute_reply": "2025-01-08T16:26:17.587069Z",
     "shell.execute_reply.started": "2025-01-08T16:26:16.947099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original video saved to /kaggle/working/Video2_Original.avi\n",
      "Segmented video saved to /kaggle/working/Video2_VanillaUNet.avi\n"
     ]
    }
   ],
   "source": [
    "# logging setup for resource monitoring\n",
    "log = {\"memory\": [], \"cpu\": [], \"gpu\": [], \"running\": True}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# starting resource monitoring in a separate thread\n",
    "monitor_thread = Thread(target=monitor_resources_continuously, args=(log,))\n",
    "monitor_thread.start()\n",
    "\n",
    "segment_left_ventricle_in_video(input_video_path, output_dir, model, device, \n",
    "                                original_video_name=original_video_name, \n",
    "                                segmented_video_name=segmented_video_name)\n",
    "\n",
    "# stopping resource monitoring after training is complete\n",
    "log[\"running\"] = False\n",
    "monitor_thread.join()\n",
    "\n",
    "# calculating training time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:26:23.903974Z",
     "iopub.status.busy": "2025-01-08T16:26:23.903660Z",
     "iopub.status.idle": "2025-01-08T16:26:23.910604Z",
     "shell.execute_reply": "2025-01-08T16:26:23.909839Z",
     "shell.execute_reply.started": "2025-01-08T16:26:23.903947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 0.64 seconds\n",
      "Average Memory Usage: 897.28 MB\n",
      "Peak Memory Usage: 897.28 MB\n",
      "Average CPU Usage: 46.30%\n",
      "Peak CPU Usage: 46.30%\n",
      "Average GPU Usage: 1.37%\n",
      "Peak GPU Usage: 1.37%\n"
     ]
    }
   ],
   "source": [
    "# recording resources used during inference\n",
    "\n",
    "if len(log[\"memory\"]) > 0:\n",
    "    avg_memory = sum(log[\"memory\"]) / len(log[\"memory\"])\n",
    "    peak_memory = max(log[\"memory\"])\n",
    "else:\n",
    "    avg_memory = peak_memory = 0\n",
    "\n",
    "if len(log[\"cpu\"]) > 0:\n",
    "    avg_cpu = sum(log[\"cpu\"]) / len(log[\"cpu\"])\n",
    "    peak_cpu = max(log[\"cpu\"])\n",
    "else:\n",
    "    avg_cpu = peak_cpu = 0\n",
    "\n",
    "if len(log[\"gpu\"]) > 0:\n",
    "    avg_gpu = sum(log[\"gpu\"]) / len(log[\"gpu\"])\n",
    "    peak_gpu = max(log[\"gpu\"])\n",
    "else:\n",
    "    avg_gpu = peak_gpu = 0\n",
    "\n",
    "print(f\"Inference Time: {inference_time:.2f} seconds\")\n",
    "print(f\"Average Memory Usage: {avg_memory:.2f} MB\")\n",
    "print(f\"Peak Memory Usage: {peak_memory:.2f} MB\")\n",
    "print(f\"Average CPU Usage: {avg_cpu:.2f}%\")\n",
    "print(f\"Peak CPU Usage: {peak_cpu:.2f}%\")\n",
    "print(f\"Average GPU Usage: {avg_gpu:.2f}%\")\n",
    "print(f\"Peak GPU Usage: {peak_gpu:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:26:45.317274Z",
     "iopub.status.busy": "2025-01-08T16:26:45.316953Z",
     "iopub.status.idle": "2025-01-08T16:26:45.321432Z",
     "shell.execute_reply": "2025-01-08T16:26:45.320457Z",
     "shell.execute_reply.started": "2025-01-08T16:26:45.317252Z"
    }
   },
   "outputs": [],
   "source": [
    "input_video_path = \"/kaggle/input/echonet-videos/EchoNet-Dynamic/Videos/0X100CF05D141FF143.avi\"\n",
    "output_dir = \"/kaggle/working\"\n",
    "original_video_name = \"Video3_Original.avi\"  # custom name for the original video\n",
    "segmented_video_name = \"Video3_VanillaUNet.avi\"  # custom name for the segmented video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:26:48.232359Z",
     "iopub.status.busy": "2025-01-08T16:26:48.232078Z",
     "iopub.status.idle": "2025-01-08T16:26:49.504875Z",
     "shell.execute_reply": "2025-01-08T16:26:49.504097Z",
     "shell.execute_reply.started": "2025-01-08T16:26:48.232338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original video saved to /kaggle/working/Video3_Original.avi\n",
      "Segmented video saved to /kaggle/working/Video3_VanillaUNet.avi\n"
     ]
    }
   ],
   "source": [
    "# logging setup for resource monitoring\n",
    "log = {\"memory\": [], \"cpu\": [], \"gpu\": [], \"running\": True}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# starting resource monitoring in a separate thread\n",
    "monitor_thread = Thread(target=monitor_resources_continuously, args=(log,))\n",
    "monitor_thread.start()\n",
    "\n",
    "segment_left_ventricle_in_video(input_video_path, output_dir, model, device, \n",
    "                                original_video_name=original_video_name, \n",
    "                                segmented_video_name=segmented_video_name)\n",
    "\n",
    "# stopping resource monitoring after training is complete\n",
    "log[\"running\"] = False\n",
    "monitor_thread.join()\n",
    "\n",
    "# calculating training time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:26:58.807282Z",
     "iopub.status.busy": "2025-01-08T16:26:58.806970Z",
     "iopub.status.idle": "2025-01-08T16:26:58.814886Z",
     "shell.execute_reply": "2025-01-08T16:26:58.814192Z",
     "shell.execute_reply.started": "2025-01-08T16:26:58.807259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 1.27 seconds\n",
      "Average Memory Usage: 897.34 MB\n",
      "Peak Memory Usage: 897.40 MB\n",
      "Average CPU Usage: 49.95%\n",
      "Peak CPU Usage: 51.20%\n",
      "Average GPU Usage: 1.37%\n",
      "Peak GPU Usage: 1.37%\n"
     ]
    }
   ],
   "source": [
    "# recording resources used during inference\n",
    "\n",
    "if len(log[\"memory\"]) > 0:\n",
    "    avg_memory = sum(log[\"memory\"]) / len(log[\"memory\"])\n",
    "    peak_memory = max(log[\"memory\"])\n",
    "else:\n",
    "    avg_memory = peak_memory = 0\n",
    "\n",
    "if len(log[\"cpu\"]) > 0:\n",
    "    avg_cpu = sum(log[\"cpu\"]) / len(log[\"cpu\"])\n",
    "    peak_cpu = max(log[\"cpu\"])\n",
    "else:\n",
    "    avg_cpu = peak_cpu = 0\n",
    "\n",
    "if len(log[\"gpu\"]) > 0:\n",
    "    avg_gpu = sum(log[\"gpu\"]) / len(log[\"gpu\"])\n",
    "    peak_gpu = max(log[\"gpu\"])\n",
    "else:\n",
    "    avg_gpu = peak_gpu = 0\n",
    "\n",
    "print(f\"Inference Time: {inference_time:.2f} seconds\")\n",
    "print(f\"Average Memory Usage: {avg_memory:.2f} MB\")\n",
    "print(f\"Peak Memory Usage: {peak_memory:.2f} MB\")\n",
    "print(f\"Average CPU Usage: {avg_cpu:.2f}%\")\n",
    "print(f\"Peak CPU Usage: {peak_cpu:.2f}%\")\n",
    "print(f\"Average GPU Usage: {avg_gpu:.2f}%\")\n",
    "print(f\"Peak GPU Usage: {peak_gpu:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:27:25.163703Z",
     "iopub.status.busy": "2025-01-08T16:27:25.163377Z",
     "iopub.status.idle": "2025-01-08T16:27:25.167589Z",
     "shell.execute_reply": "2025-01-08T16:27:25.166748Z",
     "shell.execute_reply.started": "2025-01-08T16:27:25.163679Z"
    }
   },
   "outputs": [],
   "source": [
    "input_video_path = \"/kaggle/input/echonet-videos/EchoNet-Dynamic/Videos/0X10267ADF2E644E0.avi\"\n",
    "output_dir = \"/kaggle/working\"\n",
    "original_video_name = \"Video4_Original.avi\"  # custom name for the original video\n",
    "segmented_video_name = \"Video4_VanillaUNet.avi\"  # custom name for the segmented video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:27:28.917364Z",
     "iopub.status.busy": "2025-01-08T16:27:28.917072Z",
     "iopub.status.idle": "2025-01-08T16:27:30.194255Z",
     "shell.execute_reply": "2025-01-08T16:27:30.193316Z",
     "shell.execute_reply.started": "2025-01-08T16:27:28.917343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original video saved to /kaggle/working/Video4_Original.avi\n",
      "Segmented video saved to /kaggle/working/Video4_VanillaUNet.avi\n"
     ]
    }
   ],
   "source": [
    "# logging setup for resource monitoring\n",
    "log = {\"memory\": [], \"cpu\": [], \"gpu\": [], \"running\": True}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# starting resource monitoring in a separate thread\n",
    "monitor_thread = Thread(target=monitor_resources_continuously, args=(log,))\n",
    "monitor_thread.start()\n",
    "\n",
    "segment_left_ventricle_in_video(input_video_path, output_dir, model, device, \n",
    "                                original_video_name=original_video_name, \n",
    "                                segmented_video_name=segmented_video_name)\n",
    "\n",
    "# stopping resource monitoring after training is complete\n",
    "log[\"running\"] = False\n",
    "monitor_thread.join()\n",
    "\n",
    "# calculating training time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:27:35.211306Z",
     "iopub.status.busy": "2025-01-08T16:27:35.210867Z",
     "iopub.status.idle": "2025-01-08T16:27:35.218357Z",
     "shell.execute_reply": "2025-01-08T16:27:35.217647Z",
     "shell.execute_reply.started": "2025-01-08T16:27:35.211268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 1.27 seconds\n",
      "Average Memory Usage: 897.40 MB\n",
      "Peak Memory Usage: 897.40 MB\n",
      "Average CPU Usage: 50.00%\n",
      "Peak CPU Usage: 53.70%\n",
      "Average GPU Usage: 1.37%\n",
      "Peak GPU Usage: 1.37%\n"
     ]
    }
   ],
   "source": [
    "# recording resources used during inference\n",
    "\n",
    "if len(log[\"memory\"]) > 0:\n",
    "    avg_memory = sum(log[\"memory\"]) / len(log[\"memory\"])\n",
    "    peak_memory = max(log[\"memory\"])\n",
    "else:\n",
    "    avg_memory = peak_memory = 0\n",
    "\n",
    "if len(log[\"cpu\"]) > 0:\n",
    "    avg_cpu = sum(log[\"cpu\"]) / len(log[\"cpu\"])\n",
    "    peak_cpu = max(log[\"cpu\"])\n",
    "else:\n",
    "    avg_cpu = peak_cpu = 0\n",
    "\n",
    "if len(log[\"gpu\"]) > 0:\n",
    "    avg_gpu = sum(log[\"gpu\"]) / len(log[\"gpu\"])\n",
    "    peak_gpu = max(log[\"gpu\"])\n",
    "else:\n",
    "    avg_gpu = peak_gpu = 0\n",
    "\n",
    "print(f\"Inference Time: {inference_time:.2f} seconds\")\n",
    "print(f\"Average Memory Usage: {avg_memory:.2f} MB\")\n",
    "print(f\"Peak Memory Usage: {peak_memory:.2f} MB\")\n",
    "print(f\"Average CPU Usage: {avg_cpu:.2f}%\")\n",
    "print(f\"Peak CPU Usage: {peak_cpu:.2f}%\")\n",
    "print(f\"Average GPU Usage: {avg_gpu:.2f}%\")\n",
    "print(f\"Peak GPU Usage: {peak_gpu:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:28:16.143656Z",
     "iopub.status.busy": "2025-01-08T16:28:16.143353Z",
     "iopub.status.idle": "2025-01-08T16:28:16.147329Z",
     "shell.execute_reply": "2025-01-08T16:28:16.146435Z",
     "shell.execute_reply.started": "2025-01-08T16:28:16.143634Z"
    }
   },
   "outputs": [],
   "source": [
    "input_video_path = \"/kaggle/input/echonet-videos/EchoNet-Dynamic/Videos/0X103BE163257D663A.avi\"\n",
    "output_dir = \"/kaggle/working\"\n",
    "original_video_name = \"Video5_Original.avi\"  # custom name for the original video\n",
    "segmented_video_name = \"Video5_VanillaUNet.avi\"  # custom name for the segmented video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:28:22.350380Z",
     "iopub.status.busy": "2025-01-08T16:28:22.350088Z",
     "iopub.status.idle": "2025-01-08T16:28:23.627746Z",
     "shell.execute_reply": "2025-01-08T16:28:23.626883Z",
     "shell.execute_reply.started": "2025-01-08T16:28:22.350359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original video saved to /kaggle/working/Video5_Original.avi\n",
      "Segmented video saved to /kaggle/working/Video5_VanillaUNet.avi\n"
     ]
    }
   ],
   "source": [
    "# logging setup for resource monitoring\n",
    "log = {\"memory\": [], \"cpu\": [], \"gpu\": [], \"running\": True}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# starting resource monitoring in a separate thread\n",
    "monitor_thread = Thread(target=monitor_resources_continuously, args=(log,))\n",
    "monitor_thread.start()\n",
    "\n",
    "segment_left_ventricle_in_video(input_video_path, output_dir, model, device, \n",
    "                                original_video_name=original_video_name, \n",
    "                                segmented_video_name=segmented_video_name)\n",
    "\n",
    "# stopping resource monitoring after training is complete\n",
    "log[\"running\"] = False\n",
    "monitor_thread.join()\n",
    "\n",
    "# calculating training time\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:28:29.291241Z",
     "iopub.status.busy": "2025-01-08T16:28:29.290898Z",
     "iopub.status.idle": "2025-01-08T16:28:29.298883Z",
     "shell.execute_reply": "2025-01-08T16:28:29.298223Z",
     "shell.execute_reply.started": "2025-01-08T16:28:29.291211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 1.27 seconds\n",
      "Average Memory Usage: 897.40 MB\n",
      "Peak Memory Usage: 897.40 MB\n",
      "Average CPU Usage: 49.35%\n",
      "Peak CPU Usage: 51.20%\n",
      "Average GPU Usage: 1.37%\n",
      "Peak GPU Usage: 1.37%\n"
     ]
    }
   ],
   "source": [
    "# recording resources used during inference\n",
    "\n",
    "if len(log[\"memory\"]) > 0:\n",
    "    avg_memory = sum(log[\"memory\"]) / len(log[\"memory\"])\n",
    "    peak_memory = max(log[\"memory\"])\n",
    "else:\n",
    "    avg_memory = peak_memory = 0\n",
    "\n",
    "if len(log[\"cpu\"]) > 0:\n",
    "    avg_cpu = sum(log[\"cpu\"]) / len(log[\"cpu\"])\n",
    "    peak_cpu = max(log[\"cpu\"])\n",
    "else:\n",
    "    avg_cpu = peak_cpu = 0\n",
    "\n",
    "if len(log[\"gpu\"]) > 0:\n",
    "    avg_gpu = sum(log[\"gpu\"]) / len(log[\"gpu\"])\n",
    "    peak_gpu = max(log[\"gpu\"])\n",
    "else:\n",
    "    avg_gpu = peak_gpu = 0\n",
    "\n",
    "print(f\"Inference Time: {inference_time:.2f} seconds\")\n",
    "print(f\"Average Memory Usage: {avg_memory:.2f} MB\")\n",
    "print(f\"Peak Memory Usage: {peak_memory:.2f} MB\")\n",
    "print(f\"Average CPU Usage: {avg_cpu:.2f}%\")\n",
    "print(f\"Peak CPU Usage: {peak_cpu:.2f}%\")\n",
    "print(f\"Average GPU Usage: {avg_gpu:.2f}%\")\n",
    "print(f\"Peak GPU Usage: {peak_gpu:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarizing Resources Consumed and Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:45:29.860973Z",
     "iopub.status.busy": "2025-01-08T16:45:29.860612Z",
     "iopub.status.idle": "2025-01-08T16:45:29.874334Z",
     "shell.execute_reply": "2025-01-08T16:45:29.873604Z",
     "shell.execute_reply.started": "2025-01-08T16:45:29.860952Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Video\": [\"Video 1\", \"Video 2\", \"Video 3\", \"Video 4\", \"Video 5\"],\n",
    "    \"Inference Time (s)\": [1.28, 0.64, 1.27, 1.27, 1.27],\n",
    "    \"Average Memory Usage (MB)\": [897.28, 897.28, 897.34, 897.40, 897.40],\n",
    "    \"Peak Memory Usage (MB)\": [897.28, 897.28, 897.40, 897.40, 897.40],\n",
    "    \"Average CPU Usage (%)\": [58.25, 46.30, 49.95, 50.00, 49.35],\n",
    "    \"Peak CPU Usage (%)\": [75.00, 46.30, 51.20, 53.70, 51.20],\n",
    "    \"Average GPU Usage (%)\": [1.37, 1.37, 1.37, 1.37, 1.37],\n",
    "    \"Peak GPU Usage (%)\": [1.37, 1.37, 1.37, 1.37, 1.37]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:45:55.701408Z",
     "iopub.status.busy": "2025-01-08T16:45:55.701115Z",
     "iopub.status.idle": "2025-01-08T16:45:55.727084Z",
     "shell.execute_reply": "2025-01-08T16:45:55.726303Z",
     "shell.execute_reply.started": "2025-01-08T16:45:55.701386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video</th>\n",
       "      <th>Inference Time (s)</th>\n",
       "      <th>Average Memory Usage (MB)</th>\n",
       "      <th>Peak Memory Usage (MB)</th>\n",
       "      <th>Average CPU Usage (%)</th>\n",
       "      <th>Peak CPU Usage (%)</th>\n",
       "      <th>Average GPU Usage (%)</th>\n",
       "      <th>Peak GPU Usage (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Video 1</td>\n",
       "      <td>1.28</td>\n",
       "      <td>897.28</td>\n",
       "      <td>897.28</td>\n",
       "      <td>58.25</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Video 2</td>\n",
       "      <td>0.64</td>\n",
       "      <td>897.28</td>\n",
       "      <td>897.28</td>\n",
       "      <td>46.30</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Video 3</td>\n",
       "      <td>1.27</td>\n",
       "      <td>897.34</td>\n",
       "      <td>897.40</td>\n",
       "      <td>49.95</td>\n",
       "      <td>51.2</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Video 4</td>\n",
       "      <td>1.27</td>\n",
       "      <td>897.40</td>\n",
       "      <td>897.40</td>\n",
       "      <td>50.00</td>\n",
       "      <td>53.7</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Video 5</td>\n",
       "      <td>1.27</td>\n",
       "      <td>897.40</td>\n",
       "      <td>897.40</td>\n",
       "      <td>49.35</td>\n",
       "      <td>51.2</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Video  Inference Time (s)  Average Memory Usage (MB)  \\\n",
       "0  Video 1                1.28                     897.28   \n",
       "1  Video 2                0.64                     897.28   \n",
       "2  Video 3                1.27                     897.34   \n",
       "3  Video 4                1.27                     897.40   \n",
       "4  Video 5                1.27                     897.40   \n",
       "\n",
       "   Peak Memory Usage (MB)  Average CPU Usage (%)  Peak CPU Usage (%)  \\\n",
       "0                  897.28                  58.25                75.0   \n",
       "1                  897.28                  46.30                46.3   \n",
       "2                  897.40                  49.95                51.2   \n",
       "3                  897.40                  50.00                53.7   \n",
       "4                  897.40                  49.35                51.2   \n",
       "\n",
       "   Average GPU Usage (%)  Peak GPU Usage (%)  \n",
       "0                   1.37                1.37  \n",
       "1                   1.37                1.37  \n",
       "2                   1.37                1.37  \n",
       "3                   1.37                1.37  \n",
       "4                   1.37                1.37  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:46:11.600361Z",
     "iopub.status.busy": "2025-01-08T16:46:11.600050Z",
     "iopub.status.idle": "2025-01-08T16:46:11.618838Z",
     "shell.execute_reply": "2025-01-08T16:46:11.618022Z",
     "shell.execute_reply.started": "2025-01-08T16:46:11.600338Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_averages = df.iloc[:, 1:].mean()\n",
    "\n",
    "mean_df = pd.DataFrame(mean_averages, columns=[\"Mean Average\"]).reset_index()\n",
    "mean_df.rename(columns={\"index\": \"Metric\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:46:18.263293Z",
     "iopub.status.busy": "2025-01-08T16:46:18.262838Z",
     "iopub.status.idle": "2025-01-08T16:46:18.274138Z",
     "shell.execute_reply": "2025-01-08T16:46:18.273080Z",
     "shell.execute_reply.started": "2025-01-08T16:46:18.263253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Mean Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inference Time (s)</td>\n",
       "      <td>1.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Average Memory Usage (MB)</td>\n",
       "      <td>897.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Peak Memory Usage (MB)</td>\n",
       "      <td>897.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Average CPU Usage (%)</td>\n",
       "      <td>50.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peak CPU Usage (%)</td>\n",
       "      <td>55.480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Metric  Mean Average\n",
       "0         Inference Time (s)         1.146\n",
       "1  Average Memory Usage (MB)       897.340\n",
       "2     Peak Memory Usage (MB)       897.352\n",
       "3      Average CPU Usage (%)        50.770\n",
       "4         Peak CPU Usage (%)        55.480"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:47:33.986412Z",
     "iopub.status.busy": "2025-01-08T16:47:33.985966Z",
     "iopub.status.idle": "2025-01-08T16:47:33.995453Z",
     "shell.execute_reply": "2025-01-08T16:47:33.994663Z",
     "shell.execute_reply.started": "2025-01-08T16:47:33.986384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean averages saved as 'VanillaUNet Video Segmentation Performance Metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "mean_df.to_csv(\"VanillaUNet Video Segmentation Performance Metrics.csv\", index=False)\n",
    "\n",
    "print(\"Mean averages saved as 'VanillaUNet Video Segmentation Performance Metrics.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6414327,
     "sourceId": 10357415,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6447101,
     "sourceId": 10404189,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 212069,
     "modelInstanceId": 190078,
     "sourceId": 222807,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
